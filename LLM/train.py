import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments
import bitsandbytes as bnb

print("ğŸš€ bitsandbytes ë²„ì „:", bnb.__version__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model_name = "yanolja/EEVE-Korean-Instruct-10.8B-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token  # padding í† í° ì„¤ì • (í•„ìˆ˜)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ. ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„° ìœ„ì¹˜:", next(model.parameters()).device)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. LoRA ì„¤ì • ë° ëª¨ë¸ ì¤€ë¹„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=64,
    lora_alpha=8,               # â¬…ï¸ ì¤„ì„
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.3,           # â¬†ï¸ ì¦ê°€
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
print("âœ… LoRA ì ìš© ì™„ë£Œ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ë°ì´í„°ì…‹ ë¡œë”© ë° ì „ì²˜ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
dataset = load_dataset("json", data_files="./data/instruct_with_structured_output.json")

def formatting_func(example):
    ins = example["instruction"]
    inp = example.get("input", "")
    out = example["output"]
    prompt = f"{ins}\n\n{inp}\n\n### ë‹µë³€:" if inp else f"{ins}\n\n### ë‹µë³€:"
    return {"text": f"{prompt} {out}"}

tokenized_dataset = dataset["train"].map(
    formatting_func,
    remove_columns=dataset["train"].column_names,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. í•™ìŠµ ì„¤ì • ë° ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
training_args = TrainingArguments(
    output_dir="./eeve_lora",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=1.0,  # â¬…ï¸ ì¤„ì„
    learning_rate=2e-4,
    fp16=True,
    save_steps=100,
    logging_steps=10,
    save_total_limit=2,
    optim="paged_adamw_8bit"
)

trainer = SFTTrainer(
    model=model,
    train_dataset=tokenized_dataset,
    args=training_args,
    tokenizer=tokenizer,
    packing=False,
    max_seq_length=1024,
)

trainer.train()
